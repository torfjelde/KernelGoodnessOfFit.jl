using LinearAlgebra
using Statistics

using Distributions
using ForwardDiff, ReverseDiff
using Optim

import HypothesisTests: pvalue, testname, show_params, default_tail, population_param_of_interest


struct FSSDTest <: KernelGoodnessOfFitTest
    kernel::Kernel

    # test locations
    num_test::Int
    initial_V

    # optimization
    optimize::Bool
    num_steps::Int

    # size of test
    Œ±::Float64

    # simulation under H‚ÇÄ
    num_simulate::Int
end

FSSDTest(kernel::Kernel, V::AbstractArray; Œ± = 0.01, num_simulate = 3000) = begin
    FSSDTest(kernel, size(V, 2), V, true, 50, Œ±, num_simulate)
end


struct FSSDResult <: KernelGoodnessOfFitResult
    stat::Float64
    
    p_val # p-value of the test
    Œ±
    result::Symbol # :reject or :accept

    V  # test locations
end

abstract type FSSD <: KernelGoodnessOfFitTest end

# test stuff
population_param_of_interest(t::T where T <: FSSD) = ("Finite-Set Stein Discrepancy (FSSD)", 0, t.stat)
default_tail(t::T where T <: FSSD) = :tail
show_params(io::IO, t::FSSD, ident) = begin
    println(io, ident, "kernel:           ", t.k)
    println(io, ident, "test locations:   ", t.V)
    println(io, ident, "num. simulate H‚ÇÄ: ", t.num_simulate)
end

###############
### FSSDopt ###
###############
mutable struct FSSDopt{K, A} <: FSSD where {K<:Kernel, T<:Real, A<:AbstractArray{T, 2}}
    stat::Float64             # FSSD estimate
    p_val::Float64            # p-value
    k::K                      # kernel
    V::A                      # test locations
    num_simulate::Int         # number of simulations used to approximation null-dist
    train_test_ratio::Float64 # ratio to use as TRAINING data
    # bounds::AbstractArray     # bounds for the the variables to optimize over (e.g. test locations V, kernel params)
end


function FSSDopt(
    x::AbstractArray{T1, 2},
    q::Distribution,
    k::Kernel,
    V::AbstractArray{T2, 2};
    nsim = 3000, train_test_ratio = 0.5, kwargs...
) where {T1, T2}
    n = size(x, 2)
    d, J = size(V)

    split_idx = Integer(floor(n * train_test_ratio))
    train = @view x[:, 1:split_idx]
    test = @view x[:, split_idx + 1:end]

    # update kernel parameters inplace
    kernel_params, V, opt_res = optimize_power(k, V, train, q; kwargs...)

    # update kernel
    k_new = update(k, kernel_params...)

    res = perform(k_new, V, test, q; num_simulate = nsim)
    FSSDopt(res.stat, res.p_val, k_new, V, nsim, train_test_ratio)
end

function FSSDopt(
    x::AbstractArray{T1, 1},
    q::Distribution,
    k::Kernel,
    V::AbstractArray{T2, 1};
    nsim = 3000, train_test_ratio = 0.5, kwargs...
) where {T1, T2}
    n = length(x)
    J = size(V)

    split_idx = Integer(floor(n * train_test_ratio))
    train = @view x[1:split_idx]
    test = @view x[split_idx + 1:end]

    # update kernel parameters inplace
    kernel_params, V, opt_res = optimize_power(k, V, train, q; kwargs...)

    # update kernel
    k_new = update(k, kernel_params...)

    res = perform(k_new, V, test, q; num_simulate = nsim)
    FSSDopt(res.stat, res.p_val, k_new, V, nsim, train_test_ratio)
end

FSSDopt(x::AbstractArray, q, k::Kernel; J::Int = 5, kwargs...) = begin
    # initialization
    p = fit_mle(MvNormal, x)
    V = rand(p, J)
    FSSDopt(x, q, k, V; kwargs...)
end

FSSDopt(x::AbstractArray, q; J::Int = 5, kwargs...) = begin
    # initialization
    p = fit_mle(MvNormal, x)
    V = rand(p, J)

    # default to `GaussianRBF` kernel with variance similar to MLE fit
    k = GaussianRBF(maximum(cov(p)))

    FSSDopt(x, q, k, V; kwargs...)
end

testname(::FSSDopt) = "Finite-Set Stein Discrepancy optimized (FSSD-opt)"
pvalue(t::FSSDopt) = t.p_val

################
### FSSDrand ###
################
mutable struct FSSDrand{K} <: FSSD where {K <: Kernel}
    stat::Float64     # FSSD estimate
    p_val::Float64    # p-value
    k::K              # kernel
    V::AbstractArray  # test locations
    num_simulate::Int # number of simulations used to approximation null-dist
end

# FSSDrand(x::AbstractArray, q, k::Kernel, V::AbstractArray; num_simulate = 3000) = FSSDrand(x, q, k, V, num_simulate)
# FSSDrand(x::AbstractArray, q, k::Kernel; J::Int = 5, num_simulate = 3000) = begin
#     d = size(x, 1)
#     V = zeros(d, J)
#     FSSDrand(x, q, k, V, num_simulate)
# end
# FSSDrand(x::AbstractArray, q; J::Int = 5, num_simulate = 3000) = begin
#     FSSDrand(x, q, GaussianRBF(1.0); J = J, num_simulate = num_simulate)
# end

FSSDrand(x::AbstractArray, q, k::Kernel, V::AbstractArray; nsim = 3000) = begin
    res = perform(k, V, x, q; num_simulate = nsim)
    FSSDrand(res.stat, res.p_val, k, V, nsim)
end

FSSDrand(x::AbstractArray, q, k::Kernel; J::Int = 5, kwargs...) = begin
    # initialization
    p = fit_mle(MvNormal, x)
    V = rand(p, size(x, 2))
    FSSDrand(x, q, k, V; kwargs...)
end

FSSDrand(x::AbstractArray, q; J = 5, kwargs...) = begin
    # initialization
    p = fit_mle(MvNormal, x)
    V = rand(p, J)
    k = GaussianRBF(maximum(cov(p)))

    return FSSDrand(x, q, k, V; kwargs...)
end


testname(::FSSDrand) = "Finite-Set Stein Discrepancy randomized (FSSD-rand)"
pvalue(t::FSSDrand) = t.p_val


# Objective is to maximize FSSD^2 / œÉ‚ÇÅ where

# œÉ‚ÇÅ¬≤ = 4 Œº·µÄ Œ£‚Çö Œº

# Compute unbiased estimate of covariance matrix need to compute the
# asymptotic (normal) distribution under H‚ÇÅ

# Compute ‚àá of 
function Œæ(k::Kernel, p::MultivariateDistribution, x::AbstractArray, v::AbstractArray)
    logp_dx = gradlogpdf(p, x)
    kdx = KernelGoodnessOfFit.k_dx(k, x, v)

    return logp_dx * kernel(k, x, v) + kdx
end

function Œæ(k::Kernel, p::UnivariateDistribution, x::Real, v::Real)
    logp_dx = gradlogpdf(p, x)
    kdx = KernelGoodnessOfFit.k_dx(k, x, v)

    return logp_dx * kernel(k, x, v) + kdx
end

Œû(k::Kernel, p::UnivariateDistribution, x::Real, vs::AbstractVector, J::Int=length(vs)) = Œæ.(k, p, x, vs) ./ sqrt(J)
Œû(k::Kernel, p::UnivariateDistribution, xs::AbstractVector, v::Real, J::Int) = Œæ.(k, p, xs, v) ./ sqrt(J)
function Œû(k::Kernel, p::UnivariateDistribution, xs::AbstractVector, vs::AbstractVector, J::Int=length(vs))
    return hcat([Œæ.(k, p, xs, vs[i] ./ sqrt(J)) for i = 1:J]...)
end

function Œû(k::Kernel, p::MultivariateDistribution, x::AbstractVector, vs::AbstractMatrix, J::Int=size(vs, 2))
    d = size(x, 1)
    return mapslices(v -> Œæ(k, p, x, v) / sqrt(d * J), vs; dims = 1)
end
function Œû(k::Kernel, p::MultivariateDistribution, xs::AbstractMatrix, v::AbstractVector, J::Int)
    d = size(xs, 1)
    return mapslices(x -> Œæ(k, p, x, v) / sqrt(d * J), xs; dims = 1)
end
function Œû(k::Kernel, p::MultivariateDistribution, xs::AbstractMatrix, vs::AbstractMatrix, J::Int=size(vs, 2))
    d = size(xs, 1)
    return cat([Œû(k, p, xs, vs[:, i], J) for i = 1:J]...; dims = 3)
end

function compute_Œû(k, p, xs, vs)
    # TODO: change to use `size(vs, 2)` and so on
    J = size(vs)[2]  # number of test points
    n = size(xs)[2]  # number of samples
    d = size(xs)[1]  # dimension of the inputs

    [hcat([Œæ(k, p, xs[:, i], vs[:, m]) / sqrt(d * J) for i = 1:n]...) for m = 1:J]
end

function compute_Œû(k, p::D where D <: Distribution{Univariate, Continuous}, xs, vs)
    # TODO: change to use `size(vs, 2)` and so on
    J = size(vs)[2]  # number of test points
    n = size(xs)[2]  # number of samples
    d = size(xs)[1]  # dimension of the inputs

    [hcat([Œæ(k, p, xs[1, i], vs[1, m]) / sqrt(d * J) for i = 1:n]...) for m = 1:J]
end


function fssd_from_Œû(Œû)
    J = size(Œû)[1]
    d, n = size(Œû[1])

    tot = 0.0

    # FIXME: Uhmm, this is correct buuuut definitively not O(n); it,'s O(n^2)
    for m = 1:J
        for i = 1:n - 1
            for j = i + 1:n
                tot = tot + (2 / (n * (n - 1))) * dot(Œû[m][:, i], Œû[m][:, j])
            end
        end
    end
    
    tot
end

function fssd_from_œÑ(œÑ)
    n = size(œÑ, 2)
    tmp = œÑ' * œÑ
    return (sum(tmp) - sum(diag(tmp))) / (n * (n - 1))
end

function fssd(k, p, xs, vs)
    J = size(vs)[2]  # number of test points
    n = size(xs)[2]  # number of samples
    d = size(xs)[1]  # dimension of the inputs

    tot = 0.0

    Œû_xs = [Œû(k, p, xs, vs[:, i], J) for i = 1:J]
    return fssd_from_Œû(Œû_xs)
end

function fssd_old(k, p, xs, vs)
    J = size(vs)[2]  # number of test points
    n = size(xs)[2]  # number of samples
    d = size(xs)[1]  # dimension of the inputs

    tot = 0.0
    
    for m = 1:J
        for i = 1:n - 1
            Œæ·µ¢ = Œæ(k, p, xs[:, i], vs[:, m]) ./ sqrt(d * J)
            
            for j = i + 1:n
                Œæ‚±º = Œæ(k, p, xs[:, j], vs[:, m]) ./ sqrt(d * J)

                # compute Œî(x_i, x_j)·µê, where ·µê denotes the m-th component, i.e.
                # instead of vec(Œû), we do column by column
                # => d-dim vector which we then dot
                # can think of it as œÑ(x)_{d * J: (d + 1) * J} ‚Ä¢ œÑ(y)_{d * J: (d + 1) * J} <= YOU WHAT MATE?
                # i.e. dotting the columns Œû·µ¢ ‚Ä¢ Œû‚±º 

                # TODO: should this be a dot-product?
                # should just correspond to summing over the dimensions, as we want
                tot = tot + (2 / (n * (n - 1))) * dot(Œæ·µ¢, Œæ‚±º)
            end
            
        end
    end

    # tot, Œû
    tot
end

function œÑ_from_Œû(Œû_xs)
    vcat(Œû_xs...)
end

function Œ£‚Çö(œÑ)
    dJ, n = size(œÑ)
    Œ£ = zeros((dJ, dJ))

    # take the mean over the rows, i.e. mean of observations
    Œº = mean(œÑ, dims=2)

    for i = 1:n
        Œ£ = Œ£ + (1 / n) .* œÑ[:, i] * transpose(œÑ[:, i])
    end

    # ùêÑ[œÑ(x)¬≤] - ùêÑ[œÑ(x)]¬≤
    Œ£ = Œ£ - Œº * transpose(Œº)

    Œº, Œ£
end

œÉ¬≤_H‚ÇÅ(Œº, Œ£) = begin
    # should be 1√ó1 matrix ‚Üí extract the element
    (4 * transpose(Œº) * Œ£ * Œº)[1]
end

function fssd_H‚ÇÅ_opt_factor(k, p, xs, vs; Œµ = 0.01, Œ≤_H‚ÇÅ = 0.01)
    J = size(vs, 2)
    œÑ_xs = œÑ_from_Œû([Œû(k, p, xs, vs[:, i], J) for i = 1:J])
    s = fssd_from_œÑ(œÑ_xs)
    Œº, Œ£ = Œ£‚Çö(œÑ_xs)
    œÉ‚ÇÅ = œÉ¬≤_H‚ÇÅ(Œº, Œ£)

    # asymptotic under H‚ÇÅ depends O(‚àön) on (FSSD¬≤ / œÉ‚ÇÅ¬≤ + Œµ)
    # subtract regularization term because we're going to multiply by minus
    # also, we want to stop it from being too SMALL, so regularize inverse of it
    return (s ./ (œÉ‚ÇÅ + Œµ)) .- Œ≤_H‚ÇÅ * (œÉ‚ÇÅ + (œÉ‚ÇÅ + 1e-6)^(-1))
end

function fssd_H‚ÇÅ_opt_factor(k::Kernel, p::UnivariateDistribution, xs::AbstractVector, vs::AbstractVector; Œµ = 0.01, Œ≤_H‚ÇÅ = 0.01)
    J = size(vs, 1)

    # in univariate case Œû = œÑ
    œÑ = Œû(k, p, xs, vs)

    s = fssd_from_œÑ(œÑ)

    Œº, Œ£ = Œ£‚Çö(œÑ)
    œÉ‚ÇÅ = œÉ¬≤_H‚ÇÅ(Œº, Œ£)

    return (s / (œÉ‚ÇÅ + Œµ)) .- Œ≤_H‚ÇÅ * (œÉ‚ÇÅ + (œÉ‚ÇÅ + 1e-6)^(-1))
end

### Defaults
pack(k::Kernel, vs::AbstractArray) = vcat(params(k)..., vs...)
unpack(k::Kernel, Œ∏::AbstractVector, d::Integer, J::Integer) = begin
    k_dim = length(params(k))
    return Œ∏[1:k_dim], reshape(Œ∏[k_dim + 1: end], d, J)
end

# only test-locations
pack(vs::AbstractArray) = vcat(vs...)
unpack(Œ∏::AbstractVector, d::Integer, J::Integer) = reshape(Œ∏, d, J)


### Gaussian kernel optimization: {œÉ‚Çñ, V}
pack(k::GaussianRBF, vs::AbstractArray) = vcat(log(k.gamma), vs...)
unpack(k::GaussianRBF, Œ∏::AbstractVector, d::Integer, J::Integer) = (exp(Œ∏[1]), ), reshape(Œ∏[2:end], d, J)
unpack(k::GaussianRBF, Œ∏::AbstractVector) = (exp(Œ∏[1]), ), Œ∏[2:end]


### Exponential kernel pack / unpack
pack(k::ExponentialKernel, vs::AbstractArray) = vcat(vs...)
unpack(k::ExponentialKernel, Œ∏::AbstractVector, d::Integer, J::Integer) = (), reshape(Œ∏, d, J)
unpack(k::ExponentialKernel, Œ∏::AbstractVector) = (), Œ∏

# ### Matern Kernel: do the log-exp transform to enforce positive
# pack(k::MaternKernel, vs::AbstractArray) = vcat(log(k.ŒΩ), log(k.œÅ), vs...)
# unpack(k::MaternKernel, Œ∏::AbstractArray, d::Integer, J::Integer) = exp.(Œ∏[1:2]), reshape(Œ∏[3:end], d, J)
pack(k::Matern25Kernel, vs::AbstractArray) = vcat(log(k.œÅ), vs...)
unpack(k::Matern25Kernel, Œ∏::AbstractVector, d::Integer, J::Integer) = exp.(Œ∏[1:1]), reshape(Œ∏[2:end], d, J)
unpack(k::Matern25Kernel, Œ∏::AbstractVector) = exp.(Œ∏[1:1]), Œ∏[2:end]

### InverseMultiQuadratic (IMQ): c > 0, b < 0
pack(k::InverseMultiQuadratic, vs::AbstractArray) = vcat(log(k.c), log(- k.b), vs...)
unpack(k::InverseMultiQuadratic, Œ∏::AbstractVector, d::Integer, J::Integer) = (exp(Œ∏[1]), - exp(Œ∏[2])), reshape(Œ∏[3:end], d, J)
unpack(k::InverseMultiQuadratic, Œ∏::AbstractVector) = (exp(Œ∏[1]), - exp(Œ∏[2])), Œ∏[3:end]


function make_objective(
    k::Kernel,
    p::UnivariateDistribution,
    xs::AbstractVector,
    vs::AbstractVector,
    test_locs_only::Val{onlylocs} = Val{false}();
    method::Symbol = :lbfgs,
    diff::Symbol = :forward
) where {onlylocs}
    J = size(vs, 1)

    if onlylocs
        return Œ∏ -> begin
            V = Œ∏  # nothing to reshape here so we good
            return - fssd_H‚ÇÅ_opt_factor(k, p, xs, V)
        end
    else
        return Œ∏ -> begin
            kernel_params, V = unpack(k, Œ∏)
            ker = update(k, kernel_params...)
            
            return - fssd_H‚ÇÅ_opt_factor(ker, p, xs, V)
        end
    end
end

function make_objective(
    k::Kernel,
    p::MultivariateDistribution,
    xs::AbstractMatrix,
    vs::AbstractMatrix,
    test_locs_only::Val{onlylocs} = Val{false}();
    Œ≤_H‚ÇÅ = 0.0,
    Œµ = 0.01
) where {onlylocs}
    d, n = size(xs)
    J = size(vs, 2)

    if onlylocs
        return Œ∏ -> begin
            V = unpack(Œ∏, d, J)  # nothing to reshape here so we good
            return - fssd_H‚ÇÅ_opt_factor(k, p, xs, V; Œµ = Œµ, Œ≤_H‚ÇÅ = Œ≤_H‚ÇÅ)
        end
    else
        return Œ∏ -> begin
            kernel_params, V = unpack(k, Œ∏, d, J)
            ker = update(k, kernel_params...)
            
            return - fssd_H‚ÇÅ_opt_factor(ker, p, xs, V; Œµ = Œµ, Œ≤_H‚ÇÅ = Œ≤_H‚ÇÅ)
        end
    end
end

function optimize_power(k::K, vs, xs, p; method::Symbol = :lbfgs, diff::Symbol = :forward, num_steps = 10, step_size = 0.1, Œ≤_œÉ = 0.0, Œ≤_V = 0.0, Œ≤_H‚ÇÅ = 0.0, Œµ = 0.01, lower::AbstractArray = [], upper::AbstractArray = [], test_locations_only = false) where K <: Kernel
    d, J = size(vs)

    # define objective (don't call unwrap_Œ∏ for that perf yo)
    f = make_objective(
        k, p, xs, vs, Val{test_locations_only}();
        Œ≤_H‚ÇÅ = Œ≤_H‚ÇÅ, Œµ = Œµ
    )

    # pack and combine
    Œ∏‚ÇÄ = test_locations_only ? pack(vs) : pack(k, vs)

    # define gradient
    if diff == :forward
        ‚àáf! = (F, Œ∏) -> ForwardDiff.gradient!(F, f, Œ∏)
        # ‚àáf! = nothing
    elseif diff == :backward
        ‚àáf! = (F, Œ∂) -> ReverseDiff.gradient!(F, f, Œ∂)
    elseif diff == :difference
        ‚àáf! = nothing
    else
        throw(ArgumentError("diff = $diff not not supported"))
    end

    # optimize
    if diff == :forward
        if (length(lower) > 0) && (length(upper) > 0)
            @info "optimizing using bounds"
            opt_res = optimize(f, lower, upper, clamp.(Œ∏‚ÇÄ, lower, upper), Fminbox(LBFGS()), autodiff = diff)
        else 
            opt_res = optimize(f, Œ∏‚ÇÄ, LBFGS(), autodiff = diff)
        end
    elseif diff == :backward
        if (length(lower) > 0) && (length(upper) > 0)
            @info "optimizing using bounds"
            opt_res = optimize(f, ‚àáf!, lower, upper, clamp.(Œ∏‚ÇÄ, lower, upper), Fminbox(LBFGS()))
        else 
            opt_res = optimize(f, ‚àáf!, Œ∏‚ÇÄ, LBFGS())
        end
    else
        if (length(lower) > 0) && (length(upper) > 0)
            @info "optimizing using bounds"
            opt_res = optimize(f, lower, upper, clamp.(Œ∏‚ÇÄ, lower, upper), Fminbox(LBFGS()))
        else
            opt_res = optimize(f, Œ∏‚ÇÄ, LBFGS())
        end
    end

    Œ∏ = opt_res.minimizer

    if test_locations_only
        vs_ = unpack(Œ∏, d, J)
        return (), vs_, opt_res
    else
        kernel_params_, vs_ = unpack(k, Œ∏, d, J)
        return kernel_params_, vs_, opt_res
    end
end


# 1. Choose kernel and initialize kernel parameters + test locations
# 2. [Optional] Optimize over kernel parameters + test locations
# 3. Perform test

function perform(t::FSSD)
    perform(t.k, t.V, t.x, t.q; num_simulate = t.num_simulate)
end

function perform(k::Kernel, vs, xs, p; Œ± = 0.05, num_simulate = 1000)
    d, n = size(xs)
    J = size(vs, 2)

    # compute
    œÑ_xs = œÑ_from_Œû([Œû(k, p, xs, vs[:, i], J) for i = 1:J])
    test_stat = n * fssd_from_œÑ(œÑ_xs)

    # compute asymptotics under H‚ÇÄ
    Œº, Œ£ÃÇ = Œ£‚Çö(œÑ_xs)

    # HACK: this sometimes end up with complex-valued eigenvalues (imaginary party < e^{-18}) ‚Üí conert to real
    œâ = real.(eigvals(Œ£ÃÇ))

    # simulate under H‚ÇÄ
    draws = randn(length(œâ), num_simulate)
    sim_stat = transpose(œâ) * (draws.^2 .- 1)

    # estimate P(FSSD¬≤ > \hat{FSSD¬≤}), i.e. p-value
    # FIXME: sim_stat > test_stat 100% of the time in the case where test_stat == 0.0
    # Should this be ‚â•, since the case where test_stat == 0.0 and all sim_stat == 0.0,
    # then clearly H‚ÇÄ is true, but using > we will have p-val of 0.0
    p_val = mean(sim_stat .> test_stat)

    # P(FSSD¬≤ > \hat{FSSD¬≤}) ‚â§ Œ± ‚ü∫ P(FSSD¬≤ ‚â§ \hat{FSSD¬≤}) ‚â• 1 - Œ±
    # ‚üπ reject since that means that \hat{FSSD¬≤}
    # lies outside the (1 - Œ±)-quantile ‚üπ "unlikely" for H‚ÇÄ to be true
    if p_val ‚â§ Œ±
        res = :reject
    else
        res = :accept
    end

    FSSDResult(test_stat, p_val, Œ±, res, vs)
end


function perform(t::FSSDTest, q, xs)
    return perform(t.kernel, t.initial_V, x, q; Œ± = t.Œ±, num_simulate = t.num_simulate)
end
