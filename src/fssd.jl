using LinearAlgebra
using Statistics

using Distributions
using ForwardDiff, ReverseDiff
using Optim

import HypothesisTests: pvalue, testname, show_params, default_tail, population_param_of_interest


struct FSSDTest <: GoodnessOfFitTest
    kernel::Kernel

    # test locations
    num_test::Int
    initial_V

    # optimization
    optimize::Bool
    num_steps::Int

    # size of test
    Î±::Float64

    # simulation under Hâ‚€
    num_simulate::Int
end

FSSDTest(kernel::Kernel, V::AbstractArray; Î± = 0.01, num_simulate = 3000) = begin
    FSSDTest(kernel, size(V, 2), V, true, 50, Î±, num_simulate)
end


struct FSSDResult <: GoodnessOfFitResult
    stat::Float64
    
    p_val # p-value of the test
    Î±
    result::Symbol # :reject or :accept

    V  # test locations
end

abstract type FSSD <: GoodnessOfFitTest end
set_locs!(t::FSSD, V::AbstractArray) = begin
    t.V = V
end

# test stuff
population_param_of_interest(t::T where T <: FSSD) = ("Finite-Set Stein Discrepancy (FSSD)", 0, "?")
default_tail(t::T where T <: FSSD) = :tail
show_params(io::IO, t::FSSD, ident) = begin
    println(io, ident, "kernel:           ", t.k)
    println(io, ident, "test locations:   ", t.V)
    println(io, ident, "num. simulate Hâ‚€: ", t.num_simulate)
    println(io, ident, "q:                ", t.q)
end

###############
### FSSDopt ###
###############
mutable struct FSSDopt{T} <: FSSD where T <: Kernel
    x::AbstractArray # data
    q                # a "distribution"; only requires `gradlogpdf(d, x)` to be defined
    k::T
    V::AbstractArray # have some default value
    num_simulate::Int
    train_test_ratio::Float64
end

FSSDopt(x::AbstractArray, q, k::Kernel, V::AbstractArray; num_simulate = 3000, train_test_ratio = 0.5) = FSSDopt(x, q, k, V, num_simulate, train_test_ratio)
FSSDopt(x::AbstractArray, q, k::Kernel; J::Int = 5, num_simulate = 3000, train_test_ratio = 0.5) = begin
    d = size(x, 1)
    V = zeros(d, J)
    FSSDopt(x, q, k, V, num_simulate, train_test_ratio)
end
FSSDopt(x::AbstractArray, q; J::Int = 5, num_simulate = 3000, train_test_ratio = 0.5) = begin
    FSSDopt(x, q, GaussianRBF(1.0); J = J, num_simulate = num_simulate, train_test_ratio = train_test_ratio)
end

testname(::FSSDopt) = "Finite-Set Stein Discrepancy optimized (FSSD-opt)"

################
### FSSDrand ###
################
mutable struct FSSDrand{T} <: FSSD where T  <: Kernel
    x::AbstractArray
    q
    k::T
    V::AbstractArray # have some default value
    num_simulate::Int
end

FSSDrand(x::AbstractArray, q, k::Kernel, V::AbstractArray; num_simulate = 3000) = FSSDrand(x, q, k, V, num_simulate)
FSSDrand(x::AbstractArray, q, k::Kernel; J::Int = 5, num_simulate = 3000) = begin
    d = size(x, 1)
    V = zeros(d, J)
    FSSDrand(x, q, k, V, num_simulate)
end
FSSDrand(x::AbstractArray, q; J::Int = 5, num_simulate = 3000) = begin
    FSSDrand(x, q, GaussianRBF(1.0); J = J, num_simulate = num_simulate)
end


testname(::FSSDrand) = "Finite-Set Stein Discrepancy randomized (FSSD-rand)"

# initialization of parameters
initialize!(t::FSSD) = begin
    # initialize test locations
    p = fit_mle(MvNormal, t.x)
    t.V = rand(p, size(t.V, 2))
end

initialize!(t::FSSDrand{GaussianRBF}) = begin
    # initialize test locations
    p = fit_mle(MvNormal, t.x)
    t.V = rand(p, size(t.V, 2))

    # set Gaussian kernel bandwith to maximum variance
    t.k = GaussianRBF(maximum(cov(p)))
end

pvalue(t::FSSDrand) = begin
    initialize!(t)
    
    # perform test
    res = perform(t)
    res.p_val
end

pvalue(t::FSSDopt) = begin
    initialize!(t)

    # optimize params
    optimize_power!(t)

    # perform test
    res = perform(t)

    res.p_val
end

optimize_power!(t::FSSDopt{T} where T <: Kernel) = begin
    n = size(t.x, 2)
    d, J = size(t.V)

    split_idx = Integer(floor(n * t.train_test_ratio))
    train = @view t.x[:, 1:split_idx]

    # update kernel parameters inplace
    kernel_params, V, opt_res = optimize_power(t.k, t.V, train, t.q)

    # update kernel
    set_params!(t.k, kernel_params...)

    # update test locations
    set_locs!(t, V)
end

# Objective is to maximize FSSD^2 / Ïƒâ‚ where

# Ïƒâ‚Â² = 4 Î¼áµ€ Î£â‚š Î¼

# Compute unbiased estimate of covariance matrix need to compute the
# asymptotic (normal) distribution under Hâ‚

# Compute âˆ‡ of 
function Î¾(k, p, x, v)
    # CHECKED
    
    # TODO: maybe switch to computing the Jacobian
    # logp_dx = ForwardDiff.gradient(z -> logpdf(p, z), x)
    logp_dx = gradlogpdf(p, x)
    kdx = GoodnessOfFit.k_dx(k, x, v)

    return logp_dx * kernel(k, x, v) + kdx
end


function fssd(Îž)
    J = size(Îž)[1]
    d, n = size(Îž[1])

    tot = 0.0

    # FIXME: Uhmm, this is correct buuuut definitively not O(n); it,'s O(n^2)
    for m = 1:J
        for i = 1:n - 1
            for j = i + 1:n
                tot = tot + (2 / (n * (n - 1))) * dot(Îž[m][:, i], Îž[m][:, j])
            end
        end
    end
    
    tot
end

function fssd(k, p, xs, vs)
    J = size(vs)[2]  # number of test points
    n = size(xs)[2]  # number of samples
    d = size(xs)[1]  # dimension of the inputs

    tot = 0.0

    Îž = compute_Îž(k, p, xs, vs)
    return fssd(Îž)
end

function fssd_old(k, p, xs, vs)
    J = size(vs)[2]  # number of test points
    n = size(xs)[2]  # number of samples
    d = size(xs)[1]  # dimension of the inputs

    tot = 0.0
    
    for m = 1:J
        for i = 1:n - 1
            Î¾áµ¢ = Î¾(k, p, xs[:, i], vs[:, m]) ./ sqrt(d * J)
            
            for j = i + 1:n
                Î¾â±¼ = Î¾(k, p, xs[:, j], vs[:, m]) ./ sqrt(d * J)

                # compute Î”(x_i, x_j)áµ, where áµ denotes the m-th component, i.e.
                # instead of vec(Îž), we do column by column
                # => d-dim vector which we then dot
                # can think of it as Ï„(x)_{d * J: (d + 1) * J} â€¢ Ï„(y)_{d * J: (d + 1) * J} <= YOU WHAT MATE?
                # i.e. dotting the columns Îžáµ¢ â€¢ Îžâ±¼ 

                # TODO: should this be a dot-product?
                # should just correspond to summing over the dimensions, as we want
                tot = tot + (2 / (n * (n - 1))) * dot(Î¾áµ¢, Î¾â±¼)
            end
            
        end
    end

    # tot, Îž
    tot
end

function compute_Îž(k, p, xs, vs)
    # TODO: change to use `size(vs, 2)` and so on
    J = size(vs)[2]  # number of test points
    n = size(xs)[2]  # number of samples
    d = size(xs)[1]  # dimension of the inputs

    [hcat([Î¾(k, p, xs[:, i], vs[:, m]) / sqrt(d * J) for i = 1:n]...) for m = 1:J]
end


function Ï„_from_Îž(Îž)
    vcat(Îž...)
end

function Î£â‚š(Ï„)
    dJ, n = size(Ï„)
    Î£ = zeros((dJ, dJ))

    # take the mean over the rows, i.e. mean of observations
    Î¼ = mean(Ï„, dims=2)

    for i = 1:n
        Î£ = Î£ + (1 / n) .* Ï„[:, i] * transpose(Ï„[:, i])
    end

    # ð„[Ï„(x)Â²] - ð„[Ï„(x)]Â²
    Î£ = Î£ - Î¼ * transpose(Î¼)

    Î¼, Î£
end

ÏƒÂ²_Hâ‚(Î¼, Î£) = begin
    # should be 1Ã—1 matrix â†’ extract the element
    (4 * transpose(Î¼) * Î£ * Î¼)[1]
end

function fssd_Hâ‚_opt_factor(k, p, xs, vs; Îµ = 0.01, Î²_Hâ‚ = 0.0)
    Îž = compute_Îž(k, p, xs, vs)
    s = fssd(Îž)
    Ï„ = Ï„_from_Îž(Îž)
    Î¼, Î£ = Î£â‚š(Ï„)
    Ïƒâ‚ = ÏƒÂ²_Hâ‚(Î¼, Î£)

    # asymptotic under Hâ‚ depends O(âˆšn) on (FSSDÂ² / Ïƒâ‚Â² + Îµ)
    # subtract regularization term because we're going to multiply by minus
    # also, we want to stop it from being too SMALL, so regularize inverse of it
    return (s ./ (Ïƒâ‚ + Îµ)) .- (Î²_Hâ‚ / Ïƒâ‚)
end

### Gaussian kernel optimization
function pack(k::GaussianRBF, vs::AbstractArray)
    vcat(log(k.gamma), vs...)
end

function unpack(k::GaussianRBF, Î¶::AbstractVector, d::Integer, J::Integer)
    # Ïƒâ‚–, V
    return (exp(Î¶[end]), ), reshape(Î¶[1:end - 1], d, J)
end


### Exponential kernel pack / unpack
pack(k::ExponentialKernel, vs::AbstractArray) = vcat(vs...)
unpack(k::ExponentialKernel, Î¶::AbstractVector, d::Integer, J::Integer) = (), reshape(Î¶, d, J)

# ### Matern Kernel: do the log-exp transform to enforce positive
# pack(k::MaternKernel, vs::AbstractArray) = vcat(log(k.Î½), log(k.Ï), vs...)
# unpack(k::MaternKernel, Î¶::AbstractArray, d::Integer, J::Integer) = exp.(Î¶[1:2]), reshape(Î¶[3:end], d, J)

function optimize_power(k::K, vs, xs, p; method::Symbol = :lbfgs, diff::Symbol = :backward, num_steps = 10, step_size = 0.1, Î²_Ïƒ = 0.0, Î²_V = 0.0, Î²_Hâ‚ = 0.0, Îµ = 0.01) where K <: Kernel
    d, J = size(vs)

    # define objective (don't call unwrap_Î¶ for that perf yo)
    f(Î¶) = begin
        kernel_params, V = unpack(k, Î¶, d, J)
        ker = isempty(kernel_params) ? K() : K(kernel_params...)

        # add regularization to the parameter
        # TODO: currently using matrix norm for `V` => should we use a vector for Î²_V and use vector nor?
        if Î²_Ïƒ > 0.0 || Î²_V > 0.0
            - fssd_Hâ‚_opt_factor(ker, p, xs, V; Îµ = Îµ, Î²_Hâ‚ = Î²_Hâ‚) + Î²_Ïƒ ./ (Ïƒ^2 + 1e-6) + Î²_V * norm(V)
        else
            - fssd_Hâ‚_opt_factor(ker, p, xs, V; Îµ = Îµ, Î²_Hâ‚ = Î²_Hâ‚)
        end
    end

    # define gradient
    if diff == :forward
        âˆ‡f! = (F, Î¶) -> ForwardDiff.gradient!(F, f, Î¶)
    elseif diff == :backward
        âˆ‡f! = (F, Î¶) -> ReverseDiff.gradient!(F, f, Î¶)
    elseif diff == :difference
        âˆ‡f! = nothing
    else
        throw(ArgumentError("diff = $diff not not supported"))
    end

    # pad and combine
    Î¶â‚€ = pack(k, vs)

    if method == :lbfgs
        # optimize
        if diff != :difference
            opt_res = optimize(f, âˆ‡f!, Î¶â‚€, LBFGS())
        else
            opt_res = optimize(f, Î¶â‚€, LBFGS())
        end

        Î¶ = opt_res.minimizer
        kernel_params_, vs_ = unpack(k, Î¶, d, J)
        
    elseif method == :sgd
        Î¶ = Î¶â‚€

        # setup container for gradient
        F = zeros(size(Î¶))

        # step
        opt_res = @elapsed for i = 1:num_steps
            # update
            Î¶ = Î¶ - step_size * âˆ‡f!(F, Î¶)
        end

        kernel_params_, vs_ = unpack(k, Î¶, d, J)
    else
        error("$method not recogized as a supported method")
    end

    kernel_params_, vs_, opt_res
end


# 1. Choose kernel and initialize kernel parameters + test locations
# 2. [Optional] Optimize over kernel parameters + test locations
# 3. Perform test

function perform(t::FSSD)
    perform(t.k, t.V, t.x, t.q; num_simulate = t.num_simulate)
end

function perform(k::Kernel, vs, xs, p; Î± = 0.05, num_simulate = 1000)
    d, n = size(xs)

    # compute
    Îž = compute_Îž(k, p, xs, vs)
    test_stat = n * fssd(Îž)

    # compute asymptotics under Hâ‚€
    Î¼, Î£Ì‚ = Î£â‚š(Ï„_from_Îž(Îž))

    # HACK: this sometimes end up with complex-valued eigenvalues (imaginary party < e^{-18}) â†’ conert to real
    Ï‰ = real.(eigvals(Î£Ì‚))

    # simulate under Hâ‚€
    draws = randn(length(Ï‰), num_simulate)
    sim_stat = transpose(Ï‰) * (draws.^2 .- 1)

    # estimate P(FSSDÂ² > \hat{FSSDÂ²}), i.e. p-value
    # FIXME: sim_stat > test_stat 100% of the time in the case where test_stat == 0.0
    # Should this be â‰¥, since the case where test_stat == 0.0 and all sim_stat == 0.0,
    # then clearly Hâ‚€ is true, but using > we will have p-val of 0.0
    p_val = mean(sim_stat .> test_stat)

    # P(FSSDÂ² > \hat{FSSDÂ²}) â‰¤ Î± âŸº P(FSSDÂ² â‰¤ \hat{FSSDÂ²}) â‰¥ 1 - Î±
    # âŸ¹ reject since that means that \hat{FSSDÂ²}
    # lies outside the (1 - Î±)-quantile âŸ¹ "unlikely" for Hâ‚€ to be true
    if p_val â‰¤ Î±
        res = :reject
    else
        res = :accept
    end

    FSSDResult(test_stat, p_val, Î±, res, vs)
end


function perform(t::FSSDTest, q, xs)
    return perform(t.kernel, t.initial_V, x, q; Î± = t.Î±, num_simulate = t.num_simulate)
end
